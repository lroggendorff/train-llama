import json
from trl import SFTConfig

class Config:
    def __init__(self):
        # Load config from the JSON file
        config_data = self._load_json('config.json')

        # Model and training hyperparameters
        self.BATCH_SIZE = config_data.get("batch-size")
        self.EPOCHS = config_data.get("epochs")
        self.LEARNING_RATE = config_data.get("learning-rate")
        self.MAX_SEQ_LENGTH = config_data.get("max-seq-length")
        self.VOCAB_SIZE = config_data.get("vocab-size")
        self.FP16 = config_data.get("fp16")
        self.WEIGHT_DECAY = config_data.get("weight-decay")
        self.GRADIENT_ACCUMULATION_STEPS = config_data.get("gradient-accumulation-steps")

        # Dataset configurations
        self.INPUT_DATASET = config_data.get("input-dataset")
        self.INSTRUCT_DATASET = config_data.get("instruct-dataset")
        self.SHARD_SIZE = config_data.get("shard-size")

        # Output and repo settings
        self.OUTPUT_REPO = config_data.get("output-repo")
        self.PUSH_TO_HUB = config_data.get("push-to-hub")
        self.INSTRUCT_FINETUNE_BOOL = config_data.get("instruct-finetune-bool")

        # Training steps and warmup
        self.FACTOR = config_data.get("factor")
        self.TOTAL_STEPS = (self.SHARD_SIZE * self.EPOCHS) // (self.BATCH_SIZE * self.GRADIENT_ACCUMULATION_STEPS)
        self.WARMUP_STEPS = int(self.TOTAL_STEPS * 0.1)

        # Initial state for shard offset
        self.INIT = config_data.get("init")

        # ignore
        self.getConfig = lambda: self._args()

    @staticmethod
    def _load_json(json_file):
        with open(json_file, 'r') as f:
            return json.load(f)
    
    @staticmethod
    class _AutoDict(dict):
        def __getitem__(self, key):
            return self.get(key, "auto")

    def _args(self):
        ds_config = {
            "fp16": {
                "enabled": self.FP16
            },
            "zero_optimization": {
                "stage": 2,
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "allgather_partitions": True,
                "allgather_bucket_size": 2e8,
                "overlap_comm": True,
                "reduce_scatter": True,
                "reduce_bucket_size": 2e8,
                "contiguous_gradients": True
            },
            "gradient_accumulation_steps": self.GRADIENT_ACCUMULATION_STEPS,
            "train_batch_size": self.BATCH_SIZE * self.GRADIENT_ACCUMULATION_STEPS,
            "train_micro_batch_size_per_gpu": self.BATCH_SIZE,
            "gradient_clipping": 1.0,
            "scheduler": {
                "type": "WarmupDecayLR",
                "params": {
                    "total_num_steps": self.TOTAL_STEPS,
                    "warmup_num_steps": self.WARMUP_STEPS
                }
            }
        }

        return SFTConfig(
            output_dir="model",
            num_train_epochs=self.EPOCHS,
            per_device_train_batch_size=self.BATCH_SIZE,
            learning_rate=self.LEARNING_RATE,
            warmup_steps=self.WARMUP_STEPS,
            weight_decay=self.WEIGHT_DECAY,
            gradient_accumulation_steps=self.GRADIENT_ACCUMULATION_STEPS,
            fp16=self.FP16,
            save_steps=int(self.WARMUP_STEPS * 5),
            logging_steps=max(self.BATCH_SIZE, int(self.WARMUP_STEPS)),
            save_total_limit=2,
            report_to="none",
            deepspeed=ds_config,
            max_seq_length=self.MAX_SEQ_LENGTH,
            ddp_find_unused_parameters=False
        )
